{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 2. Twitter Data ETL : Advanced Project in Digital Media Engineering\n",
    "\n",
    "- pervious chapter 1, about NOAA Data ETL, can be redirected from the link __[here](http://nbviewer.jupyter.org/github/ShawnHouCHN/Advanced-Project-in-Digital-Media-Engineering-E17/blob/master/notebooks/USA%20climatological%20station%20data%20parser.ipynb)__ \n",
    "\n",
    "- next chapter 3, about data analysis, can be redirected from the link __[here](http://nbviewer.jupyter.org/github/ShawnHouCHN/Advanced-Project-in-Digital-Media-Engineering-E17/blob/master/notebooks/USA%20twitter-weather%20data%20analysis%20.ipynb)__ \n",
    "***\n",
    "In this chapter, the notebook will show the extraction, transfer and Some visualizations have been made to give a intuitvie insights of the result. <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# %load extract_weather_keywords.py\n",
    "#!/usr/bin/env python2\n",
    "\"\"\"\n",
    "Created on Wed Oct 11 15:10:40 2017\n",
    "\n",
    "@author: omd\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Docs :)\n",
    "# 0 tweetid\n",
    "# 1 userid\n",
    "# 2 timestamp\n",
    "# 3 reply-tweetid\n",
    "# 4 reply-userid\n",
    "# 5 source\n",
    "# 6 truncated?\n",
    "# 7 geo-tag\n",
    "# 8 location\n",
    "# 9 tweet-text\n",
    "# 10 twittername (text)\n",
    "# 11 twittername (handle)\n",
    "\n",
    "# Determine whether code is running on the cluster or locally (for testing)\n",
    "import socket\n",
    "achtung = 'achtung' in socket.gethostname()\n",
    "# If testing locally, find the Spark instalation\n",
    "if not achtung:\n",
    "    import findspark\n",
    "    findspark.init()\n",
    "import datetime\n",
    "from pyspark import SparkContext\n",
    "import os\n",
    "import numpy as np\n",
    "import shutil\n",
    "import time\n",
    "# If cluster, import stuff to interact with Hadoop file system\n",
    "if achtung:\n",
    "    import pydoop.hdfs as hdfs\n",
    "\n",
    "from pushover import Client\n",
    "\n",
    "from copy import deepcopy\n",
    "from dateutil.parser import parse\n",
    "import json\n",
    "import re\n",
    "\n",
    "def pushme(msg, title = \"\"):\n",
    "    Client().send_message(msg, title=title)\n",
    "\n",
    "######################################################################\n",
    "APP_NAME = \"Weather keyword extractor\"\n",
    "\n",
    "# Fraction of data to load\n",
    "sample_fraction = 1.0\n",
    "SAVE_LOCAL = False  # Whether to extract data from HDFS\n",
    "FILES = \"tweets.2016-*\" # File glob thingy to read in on cluster\n",
    "outname = \"weather_keyword_counts.tsv\"\n",
    "######################################################################\n",
    "\n",
    "if achtung:\n",
    "    outpath = outname  #\"/net/data/bjarkemoensted/\"+outname\n",
    "else:\n",
    "    outpath = outname\n",
    "\n",
    "if achtung:\n",
    "    path = \"hdfs://megatron.ccs.neu.edu/user/amislove/twitter/gardenhose-data/summarized/\"+FILES\n",
    "else:\n",
    "    path = \"sample.txt\"\n",
    "\n",
    "def normalize(loc, state_codes_b):\n",
    "    '''Normalizes location strings. Little Bjarke wrote this.'''\n",
    "    \n",
    "    # remove unicode\n",
    "    loc = loc.encode('unicode-escape')\n",
    "    loc = loc.decode('unicode_escape').encode('ascii','ignore')    \n",
    "\n",
    "    # remove non-alphanumeric\n",
    "    loc = loc.lower()\n",
    "    loc = re.sub('[^0-9a-zA-Z]+', ' ', loc)\n",
    "\n",
    "    # remove US from right side\n",
    "    loc = ' '.join(loc.split())\n",
    "    us_names = [' us', ' usa', ' united states', ' united states of america']\n",
    "    for pattern in us_names:\n",
    "        loc = re.sub('%s$' % pattern, '', loc)\n",
    "\n",
    "    # abbreviate state names\n",
    "    for s in state_codes_b.value:\n",
    "        loc = loc.replace(s[0], ' '+s[1])\n",
    "\n",
    "    # cleaning whitespace uses\n",
    "    loc = ' '.join(loc.split())\n",
    "    loc = loc.strip()\n",
    "\n",
    "    if loc == '':\n",
    "        return None\n",
    "    return loc\n",
    "\n",
    "def process(rawline, keyword_map_b):\n",
    "    '''Converts a tweet entry into a list. Returns None if error.'''\n",
    "    \n",
    "    try:\n",
    "        if isinstance(rawline, str):\n",
    "            line = rawline.decode('utf-8')\n",
    "        elif isinstance(rawline, unicode):\n",
    "            line = rawline\n",
    "        else:\n",
    "            raise TypeError(\"Line neither string nor unicode\")\n",
    "        entries = line.split(\"\\t\")\n",
    "        if len(entries) != 12:\n",
    "            return None\n",
    "        \n",
    "        # text\n",
    "        text = entries[9]\n",
    "        words = text.lower().split()\n",
    "#         for i in range(len(words)-1, -1, -1):\n",
    "#             if (re.match(url_pattern, words[i])\n",
    "#                 or words[i] == 'rt'\n",
    "#                 or words[i].startswith(\"@\")):\n",
    "#                 del words[i]\n",
    "\n",
    "\n",
    "        cleaned = [\"\".join(ch for ch in w if ch.isalnum() or ch in set(\"-%/\\\\#@\")) for w in words]\n",
    "        matched = set([])\n",
    "        for word in cleaned:\n",
    "            try:\n",
    "                matched.add(keyword_map_b.value[word])\n",
    "            except KeyError:\n",
    "                continue\n",
    "            #\n",
    "        \n",
    "        if len(matched) == 0:\n",
    "            return None\n",
    "        \n",
    "        \n",
    "        # time\n",
    "        ts = parse(entries[2])\n",
    "        ymd = \"_\".join([str(ts.year).zfill(4)]+list(map(lambda x: str(x).zfill(2), (ts.month, ts.day))))\n",
    "        \n",
    "        return ymd, matched, entries[8]\n",
    "    except:\n",
    "        print \"Failed. Tweet entries:\", len(rawline.split(\"\\t\"))\n",
    "        return None\n",
    "\n",
    "def combine_dicts(a, b):\n",
    "    result = deepcopy(a)\n",
    "    for keyword, pdist in b.iteritems():\n",
    "        if not (keyword in result):\n",
    "            result[keyword] = deepcopy(pdist)\n",
    "        else:\n",
    "            for county, prob in pdist.iteritems():\n",
    "                try:\n",
    "                    result[keyword][county] += prob\n",
    "                except KeyError:\n",
    "                    result[keyword][county] = prob\n",
    "                #\n",
    "            #\n",
    "        #\n",
    "    return result\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "\n",
    "    with open(\"keyword_map.json\") as f:\n",
    "        keyword_map = json.load(f)\n",
    "    \n",
    "    # Create sparkcontext (duh)\n",
    "    sc = SparkContext(appName = APP_NAME)\n",
    "    \n",
    "    keyword_map_b = sc.broadcast(keyword_map)\n",
    "    state_codes = np.loadtxt('cont_states.tsv', delimiter='\\t', skiprows=1, usecols=[0,1], dtype='|S25')\n",
    "    state_codes = np.char.lower(state_codes)\n",
    "    state_codes_b = sc.broadcast(state_codes)\n",
    "\n",
    "    with open(\"normalized_loc_string2p_dist.json\") as f:\n",
    "        loc_string2p_dist = json.load(f)    \n",
    "    loc_string2p_dist_b = sc.broadcast(loc_string2p_dist)\n",
    "    \n",
    "    ymd_matches_rawloc = sc.textFile(path).map(lambda line: process(line, keyword_map_b)).filter(lambda x: x is not None)\n",
    "    ymd_matches_normloc = ymd_matches_rawloc.map(lambda t: (t[0], t[1], normalize(t[2], state_codes_b))).filter(lambda t: t[2] is not None)\n",
    "    \n",
    "    data_with_loc_data = ymd_matches_normloc.filter(lambda t: t[2] in loc_string2p_dist_b.value)\n",
    "    ymd_dicts = data_with_loc_data.map(lambda t: (t[0], {w : loc_string2p_dist_b.value[t[2]] for w in t[1]}))\n",
    "    ymd_combined = ymd_dicts.reduceByKey(lambda a,b: combine_dicts(a,b))\n",
    "    \n",
    "    ordered = sorted(ymd_combined.collect(), key = lambda t: t[0])\n",
    "    \n",
    "    # Stop spark instance\n",
    "    sc.stop()\n",
    "    \n",
    "    with open(outpath, \"w\") as f:\n",
    "        for date, dict_ in ordered:\n",
    "            f.write(date+\"\\t\"+json.dumps(dict_)+\"\\n\")\n",
    "    \n",
    "    dt = int(time.time() - start_time)\n",
    "    timestring = str(datetime.timedelta(seconds = dt))\n",
    "    \n",
    "    # If running on cluster, send push notification to my phone.\n",
    "    if achtung:\n",
    "        pushme(\"Finished running \"+os.path.split(__file__)[-1] + \n",
    "               \"\\nRuntime: \"+timestring)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
